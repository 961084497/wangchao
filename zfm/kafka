__________________________________________________________________
! potation01,02    |	potation01,02	|	potation01,02     |
!	大         | 集        群   单  |       机     器         |
!    topic01	   |	topic02		|	topic03		  |
!_________________________________________________________________|




kafka 自带的zookeeper我就感觉很好了 没必要重新安装zookeeper

Broker ： 安装Kafka服务的那台集群就是一个broker（broker的id要全局唯一）  每个broker对应一台服务器 (三台集群 就有3个broke)
Producer：消息的生产者，负责将数据写入到broker中（push）
Consumer：消息的消费者，负责从kafka中读取数据（pull），老版本的消费者需要依赖zk，新版本的不需要
Topic:主题，相当于是数据的一个分类，不同topic存放不同的数据
replication：副本，数据保存多少份（保证数据不丢）
partition：分区，是一个物理分区，一个分区就是一个文件，一个topic可以有一到多个分区，每一个分区都有自己的副本。
Consumer Group：消费者组，一个topic可以有多个消费者同时消费，多个消费者如果在一个消费者组中，那么他们不能重复消费数据


比较好的一篇文章 https://blog.csdn.net/weixin_43866709/article/details/88989349

-----------------------------   kafka  可能会丢失一些数据  下面就是举例  -------------------------------
唯一可能导致消费者弄丢数据的情况，就是说，你那个消费到了这个消息，然后消费者那边自动提交了offset，让kafka以为你已经消费好了这个消息，其实你刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。

这不是一样么，大家都知道kafka会自动提交offset，那么只要关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。但是此时确实还是会重复消费，比如你刚处理完，还没提交offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。

生产环境碰到的一个问题，就是说我们的kafka消费者消费到了数据之后是写到一个内存的queue里先缓冲一下，结果有的时候，你刚把消息写入内存queue，然后消费者会自动提交offset。

然后此时我们重启了系统，就会导致内存queue里还没来得及处理的数据就丢失了

----------------------------------------------  重点来了  ----------------------------------------------------------------

同一个分区内的消息只能被同一个组中的一个消费者消费，当消费者数量多于分区数量时，多于的消费者空闲（不能消费数据）。
组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。当然，每个分区只能由同一个消费组内的一个consumer来消费。

上面2句话的意思就是说 创建一个Consumer Group 是消费整个topic里面的数据的 消费组里面的每个消费者针对着一个分区 所以多个消费组肯定是会消费重复数据的(不同的消费组也就是id不一样)

同一个Consumer Group 可以分布在不同的机器 如果是2个Consumer Group那肯定会重复消费   但是增加Consumer Group但是是相同的Consumer Group id那就是相互抢资源 你在工作 别人只能休息(多个机器造成多个消费组,但是id一样)




-----------------docker 安装 kafka ---------------------
这里使用了wurstmeister/kafka和wurstmeister/zookeeper这两个版本的镜像

docker pull wurstmeister/zookeeper

docker pull wurstmeister/kafka

docker pull sheepkiller/kafka-manager

在命令中运行docker images验证三个镜像已经安装完毕

docker run -d --name zookeeper --publish 2181:2181 \--volume /etc/localtime:/etc/localtime \--restart=always \wurstmeister/zookeeper

docker run -d --name kafka --publish 9082:9092 \--link zookeeper:zookeeper \--env KAFKA_BROKER_ID=100 \--env HOST_IP=192.168.1.108 \--env KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \--env KAFKA_ADVERTISED_HOST_NAME=192.168.1.108 \--env KAFKA_ADVERTISED_PORT=9082 \--restart=always \--volume /etc/localtime:/etc/localtime \wurstmeister/kafka

docker run -d --name kafka-manager \--link zookeeper:zookeeper \--link kafka:kafka -p 9001:9000 \--restart=always \--env ZK_HOSTS=zookeeper:2181 \sheepkiller/kafka-manager

http://192.168.1.108:9001/
